{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f39c3e42-0cff-44ee-bf8e-b2150b205426",
   "metadata": {},
   "source": [
    "## A notebook containing training logic for a robot to lift a cube using the mpo algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b71e82c1-7d92-4231-8c5f-2ba067a8c674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include all the imports here\n",
    "from typing import Dict, Sequence\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import acme\n",
    "from acme import specs\n",
    "from acme import types\n",
    "from acme import wrappers\n",
    "from acme.agents.tf import dmpo\n",
    "from acme.tf import networks\n",
    "from acme.tf import utils as tf2_utils\n",
    "\n",
    "import numpy as np\n",
    "import sonnet as snt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7254f20e-a2d9-47f8-87a0-2d8d954c95ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import robosuite as suite\n",
    "from robosuite.wrappers import GymWrapper\n",
    "from robosuite.controllers import load_controller_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f21e7feb-b681-4990-8d14-4e947a9209cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the environment\n",
    "env_config = {\n",
    "     \"control_freq\": 20,\n",
    "    \"env_name\": \"Lift\",\n",
    "    \"hard_reset\": False,\n",
    "    \"horizon\": 500,\n",
    "    \"ignore_done\": False,\n",
    "    \"reward_scale\": 1.0,\n",
    "    \"camera_names\": \"frontview\",\n",
    "    \"robots\": [\n",
    "      \"Panda\"\n",
    "    ]\n",
    "}\n",
    "controller_config = load_controller_config(default_controller=\"OSC_POSITION\")\n",
    "\n",
    "keys = [\"object-state\"]\n",
    "for idx in range(1):\n",
    "    keys.append(f\"robot{idx}_proprio-state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbb3f0d3-fdb6-43bc-bcfe-d7aeae1ab8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_environment(env_config, controller_config, keys):\n",
    "    env_suite = suite.make(**env_config,\n",
    "                 has_renderer=False,\n",
    "                 has_offscreen_renderer=False,\n",
    "                 use_camera_obs=False,\n",
    "                 reward_shaping=True,\n",
    "                 controller_configs=controller_config,\n",
    "                 )\n",
    "    env = GymWrapper(env_suite, keys=keys)\n",
    "    env = wrappers.gym_wrapper.GymWrapper(env)\n",
    "    env = wrappers.SinglePrecisionWrapper(env)\n",
    "    \n",
    "    spec = specs.make_environment_spec(env)\n",
    "    \n",
    "    return env, spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db302a65-4883-4c30-9046-81971b35c717",
   "metadata": {},
   "outputs": [],
   "source": [
    "env, spec = make_environment(env_config, controller_config, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b50098f-3eb0-46d1-ae17-8560a7caa844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44b534ca-ce24-4343-96ee-a56927446812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the agent\n",
    "\n",
    "def make_networks(\n",
    "    action_spec: specs.BoundedArray,\n",
    "    policy_layer_sizes: Sequence[int] = (256, 256, 256),\n",
    "    critic_layer_sizes: Sequence[int] = (512, 512, 256),\n",
    "    vmin: float = -150.,\n",
    "    vmax: float = 150.,\n",
    "    num_atoms: int = 51,\n",
    ") -> Dict[str, types.TensorTransformation]:\n",
    "      \"\"\"Creates networks used by the agent.\"\"\"\n",
    "\n",
    "      # Get total number of action dimensions from action spec.\n",
    "      num_dimensions = np.prod(action_spec.shape, dtype=int)\n",
    "\n",
    "      # Create the shared observation network; here simply a state-less operation.\n",
    "      observation_network = tf2_utils.batch_concat\n",
    "\n",
    "      # Create the policy network.\n",
    "      policy_network = snt.Sequential([\n",
    "          networks.LayerNormMLP(policy_layer_sizes),\n",
    "          networks.MultivariateNormalDiagHead(num_dimensions)\n",
    "      ])\n",
    "\n",
    "      # The multiplexer transforms concatenates the observations/actions.\n",
    "      multiplexer = networks.CriticMultiplexer(\n",
    "          critic_network=networks.LayerNormMLP(critic_layer_sizes),\n",
    "          action_network=networks.ClipToSpec(action_spec))\n",
    "\n",
    "      # Create the critic network.\n",
    "      critic_network = snt.Sequential([\n",
    "          multiplexer,\n",
    "          networks.DiscreteValuedHead(vmin, vmax, num_atoms),\n",
    "      ])\n",
    "\n",
    "      return {\n",
    "          'policy': policy_network,\n",
    "          'critic': critic_network,\n",
    "          'observation': observation_network,\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "278ef2e1-6205-466b-bcfc-e75af0dfbaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_networks = make_networks(spec.actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2dc1dc68-d5ae-4d74-b707-3e95e8e0c1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mohan/miniconda3/envs/acme/lib/python3.8/site-packages/tensorflow_probability/python/distributions/distribution.py:345: calling MultivariateNormalDiag.__init__ (from tensorflow_probability.python.distributions.mvn_diag) with scale_identity_multiplier is deprecated and will be removed after 2020-01-01.\n",
      "Instructions for updating:\n",
      "`scale_identity_multiplier` is deprecated; please combine it into `scale_diag` directly instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mohan/miniconda3/envs/acme/lib/python3.8/site-packages/tensorflow_probability/python/distributions/distribution.py:345: calling MultivariateNormalDiag.__init__ (from tensorflow_probability.python.distributions.mvn_diag) with scale_identity_multiplier is deprecated and will be removed after 2020-01-01.\n",
      "Instructions for updating:\n",
      "`scale_identity_multiplier` is deprecated; please combine it into `scale_diag` directly instead.\n"
     ]
    }
   ],
   "source": [
    "# construct the agent\n",
    "agent = dmpo.DistributionalMPO(\n",
    "      environment_spec=spec,\n",
    "      policy_network=agent_networks['policy'],\n",
    "      critic_network=agent_networks['critic'],\n",
    "      observation_network=agent_networks['observation'],  # pytype: disable=wrong-arg-types\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f3d6836-0cf2-4b3e-a20d-f7233cb10d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the training process\n",
    "loop = acme.EnvironmentLoop(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3249b026-07c7-44ac-9ef6-0c4d3b201ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57b56b66-eb84-414d-b5b0-0718cc7eabd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mohan/miniconda3/envs/acme/lib/python3.8/site-packages/tensorflow_probability/python/distributions/distribution.py:345: calling Independent.__init__ (from tensorflow_probability.python.distributions.independent) with reinterpreted_batch_ndims=None is deprecated and will be removed after 2022-03-01.\n",
      "Instructions for updating:\n",
      "Please pass an integer value for `reinterpreted_batch_ndims`. The current behavior corresponds to `reinterpreted_batch_ndims=tf.size(distribution.batch_shape_tensor()) - 1`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mohan/miniconda3/envs/acme/lib/python3.8/site-packages/tensorflow_probability/python/distributions/distribution.py:345: calling Independent.__init__ (from tensorflow_probability.python.distributions.independent) with reinterpreted_batch_ndims=None is deprecated and will be removed after 2022-03-01.\n",
      "Instructions for updating:\n",
      "Please pass an integer value for `reinterpreted_batch_ndims`. The current behavior corresponds to `reinterpreted_batch_ndims=tf.size(distribution.batch_shape_tensor()) - 1`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/mohan/acme/05b572d2-c46c-11eb-9e00-d7ad18c91467/snapshots/policy/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/mohan/acme/05b572d2-c46c-11eb-9e00-d7ad18c91467/snapshots/policy/assets\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-e4f940a3a32e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/research/masters/robotics/rl/acme/acme/environment_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, num_episodes, num_steps)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0mepisode_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshould_terminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m       \u001b[0mepisode_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m       \u001b[0mstep_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'episode_length'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/masters/robotics/rl/acme/acme/environment_loop.py\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0;31m# Generate an action from the agent's policy and step the environment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m       \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_actor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtimestep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_environment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m       \u001b[0;31m# Have the agent observe the timestep and let the actor update itself.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/masters/robotics/rl/acme/acme/wrappers/single_precision.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdm_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeStep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_timestep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_environment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdm_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeStep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/masters/robotics/rl/acme/acme/wrappers/gym_wrapper.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     55\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_environment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset_next_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/masters/robotics/environments/robosuite/robosuite/wrappers/gym_wrapper.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mmisc\u001b[0m \u001b[0minformation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \"\"\"\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mob_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flatten_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mob_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/masters/robotics/environments/robosuite/robosuite/environments/base.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;31m# (as defined by the control frequency specified at the environment level)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_timestep\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_timestep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pre_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loop.run(num_episodes=num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa1a99b-8de8-46a6-bd91-6c736ad77b78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
